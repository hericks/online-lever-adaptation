{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd29ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path\n",
    "sys.path.insert(1, path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85779e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5583d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c67eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from levers import IteratedLeverEnvironment\n",
    "from levers.partners import FixedPatternPartner\n",
    "from levers.learner import DQNAgent, HistoryShaper, Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0485a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "n_eval_mc = 1\n",
    "n_train_mc = 3\n",
    "\n",
    "# Final results table\n",
    "# Format: eval_id, train_id, test_pattern_id, train_patterns_id\n",
    "scores = torch.zeros((70, 8, n_train_mc, n_eval_mc))\n",
    "greedy_scores = torch.zeros((70, 8, n_train_mc, n_eval_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6c0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_online_q_learner(train_patterns, test_pattern, train_id):\n",
    "    # Environment settings\n",
    "    payoffs = [1., 1.]\n",
    "    truncated_length = 100\n",
    "    include_step = False\n",
    "    include_payoffs = False\n",
    "\n",
    "    # Construct environment\n",
    "    env = IteratedLeverEnvironment(\n",
    "        payoffs, truncated_length+1, FixedPatternPartner(test_pattern),\n",
    "        include_step, include_payoffs\n",
    "    )\n",
    "\n",
    "\n",
    "    # History shaper settings\n",
    "    hs_hidden_size = 4\n",
    "\n",
    "    # Construct history shaper\n",
    "    hist_shaper = HistoryShaper(\n",
    "        hs_net=nn.LSTM(\n",
    "            input_size=len(env.dummy_obs()),\n",
    "            hidden_size=hs_hidden_size,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Load history shaper from experiment\n",
    "    experiment_name = 'online_qlearner_all_length3_manual'\n",
    "    data_dir = 'data'\n",
    "    model_name = 'hs-net-pattern={tps}-eval_id={eid:02d}.pt'.format(\n",
    "        tps=train_patterns,\n",
    "        eid=train_id\n",
    "    )\n",
    "    model_path = path.join(experiment_name, data_dir, model_name)\n",
    "    hist_shaper.net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "    # Learner settings\n",
    "    learner_hidden_size = 4\n",
    "    capacity = 16\n",
    "    batch_size = 8\n",
    "    lr = 0.01\n",
    "    gamma = 0.99\n",
    "    len_update_cycle = 10\n",
    "\n",
    "    # Initialize DQN agent\n",
    "    learner = DQNAgent(\n",
    "        q_net=nn.Sequential(\n",
    "            nn.Linear(hs_hidden_size, learner_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(learner_hidden_size, env.n_actions()),\n",
    "        ),\n",
    "        capacity=capacity,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        gamma=gamma,\n",
    "        len_update_cycle=len_update_cycle\n",
    "    )\n",
    "\n",
    "    # Load q-net from experiment\n",
    "    experiment_name = 'online_qlearner_all_length3_manual'\n",
    "    data_dir = 'data'\n",
    "    model_name = 'q-net-pattern={tps}-eval_id={eid:02d}.pt'.format(\n",
    "        tps=train_patterns,\n",
    "        eid=train_id\n",
    "    )\n",
    "    model_path = path.join(experiment_name, data_dir, model_name)\n",
    "    learner.q_net.load_state_dict(torch.load(model_path))\n",
    "    learner.reset()\n",
    "\n",
    "    ret, greedy_ret, greedy_steps = 0, 0, 0\n",
    "    obs = env.reset()\n",
    "    obs_rep, hidden = hist_shaper.net(obs.unsqueeze(0))\n",
    "    for step in range(truncated_length):\n",
    "        epsilon = 1 * (1 - 4 * step / truncated_length)\n",
    "        action, is_greedy = learner.act(obs_rep.squeeze(0), epsilon=epsilon)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "\n",
    "        ret += reward\n",
    "        greedy_ret += reward if is_greedy else 0\n",
    "        greedy_steps += is_greedy\n",
    "\n",
    "        # Compute history representation\n",
    "        next_obs_rep, next_hidden = hist_shaper.net(\n",
    "            next_obs.unsqueeze(0), hidden)\n",
    "\n",
    "        # Give experience to learner and train\n",
    "        learner.update_memory(\n",
    "            Transition(\n",
    "                obs_rep.squeeze(0).detach(),\n",
    "                action, \n",
    "                next_obs_rep.squeeze(0).detach(), \n",
    "                reward, done\n",
    "            )\n",
    "        )\n",
    "        learner.train(done)\n",
    "\n",
    "        # Update next observation -> observation\n",
    "        obs = next_obs\n",
    "        obs_rep = next_obs_rep\n",
    "        hidden = next_hidden\n",
    "\n",
    "    score = ret / truncated_length\n",
    "    greedy_score = greedy_ret / greedy_steps\n",
    "    return score, greedy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc7c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      " 0/70 | (0, 0, 0) (*): actual: [0.942 0.932 0.932], greedy: [1. 1. 1.]\n",
      " 0/70 | (0, 0, 1) (*): actual: [0.912 0.884 0.756], greedy: [0.971 0.956 0.805]\n",
      " 0/70 | (0, 1, 0) (*): actual: [0.862 0.908 0.76 ], greedy: [0.93  0.955 0.798]\n",
      " 0/70 | (0, 1, 1) (*): actual: [0.8   0.88  0.734], greedy: [0.847 0.931 0.767]\n",
      " 0/70 | (1, 0, 0) ( ): actual: [0.866 0.896 0.744], greedy: [0.931 0.952 0.789]\n",
      " 0/70 | (1, 0, 1) ( ): actual: [0.794 0.826 0.758], greedy: [0.847 0.892 0.8  ]\n",
      " 0/70 | (1, 1, 0) ( ): actual: [0.808 0.854 0.792], greedy: [0.851 0.902 0.842]\n",
      " 0/70 | (1, 1, 1) ( ): actual: [0.92  0.926 0.884], greedy: [0.987 0.991 0.948]\n",
      "Actual-train:  0.86, Actual-test:  0.84\n",
      "Greedy-train:  0.91, Greedy-test:  0.89\n",
      "--------------------------------------------------\n",
      " 1/70 | (0, 0, 0) (*): actual: [0.942 0.93  0.948], greedy: [1. 1. 1.]\n",
      " 1/70 | (0, 0, 1) (*): actual: [0.748 0.914 0.742], greedy: [0.777 0.972 0.785]\n",
      " 1/70 | (0, 1, 0) (*): actual: [0.812 0.914 0.722], greedy: [0.836 0.961 0.746]\n",
      " 1/70 | (0, 1, 1) ( ): actual: [0.744 0.878 0.848], greedy: [0.773 0.927 0.876]\n",
      " 1/70 | (1, 0, 0) (*): actual: [0.712 0.876 0.696], greedy: [0.746 0.955 0.738]\n",
      " 1/70 | (1, 0, 1) ( ): actual: [0.672 0.864 0.79 ], greedy: [0.69  0.921 0.84 ]\n",
      " 1/70 | (1, 1, 0) ( ): actual: [0.71  0.866 0.75 ], greedy: [0.747 0.922 0.796]\n",
      " 1/70 | (1, 1, 1) ( ): actual: [0.902 0.918 0.912], greedy: [0.979 0.993 0.975]\n",
      "Actual-train:  0.83, Actual-test:  0.82\n",
      "Greedy-train:  0.88, Greedy-test:  0.87\n",
      "--------------------------------------------------\n",
      " 2/70 | (0, 0, 0) (*): actual: [0.944 0.93  0.932], greedy: [1. 1. 1.]\n",
      " 2/70 | (0, 0, 1) (*): actual: [0.806 0.752 0.794], greedy: [0.847 0.789 0.839]\n",
      " 2/70 | (0, 1, 0) (*): actual: [0.82  0.81  0.706], greedy: [0.851 0.856 0.736]\n",
      " 2/70 | (0, 1, 1) ( ): actual: [0.676 0.714 0.694], greedy: [0.683 0.753 0.723]\n",
      " 2/70 | (1, 0, 0) ( ): actual: [0.788 0.758 0.682], greedy: [0.834 0.795 0.725]\n",
      " 2/70 | (1, 0, 1) (*): actual: [0.658 0.69  0.708], greedy: [0.678 0.726 0.733]\n",
      " 2/70 | (1, 1, 0) ( ): actual: [0.626 0.628 0.684], greedy: [0.648 0.648 0.717]\n",
      " 2/70 | (1, 1, 1) ( ): actual: [0.928 0.896 0.916], greedy: [0.986 0.961 0.973]\n",
      "Actual-train:  0.80, Actual-test:  0.75\n",
      "Greedy-train:  0.84, Greedy-test:  0.79\n",
      "--------------------------------------------------\n",
      " 3/70 | (0, 0, 0) (*): actual: [0.936 0.92  0.934], greedy: [1. 1. 1.]\n",
      " 3/70 | (0, 0, 1) (*): actual: [0.908 0.752 0.9  ], greedy: [0.961 0.791 0.943]\n",
      " 3/70 | (0, 1, 0) (*): actual: [0.88  0.69  0.818], greedy: [0.928 0.72  0.868]\n",
      " 3/70 | (0, 1, 1) ( ): actual: [0.912 0.644 0.698], greedy: [0.953 0.669 0.727]\n",
      " 3/70 | (1, 0, 0) ( ): actual: [0.886 0.732 0.822], greedy: [0.954 0.764 0.875]\n",
      " 3/70 | (1, 0, 1) ( ): actual: [0.888 0.678 0.66 ], greedy: [0.963 0.713 0.689]\n",
      " 3/70 | (1, 1, 0) (*): actual: [0.862 0.646 0.62 ], greedy: [0.907 0.671 0.651]\n",
      " 3/70 | (1, 1, 1) ( ): actual: [0.922 0.926 0.932], greedy: [0.98  0.966 0.986]\n",
      "Actual-train:  0.82, Actual-test:  0.81\n",
      "Greedy-train:  0.87, Greedy-test:  0.85\n",
      "--------------------------------------------------\n",
      " 4/70 | (0, 0, 0) (*): actual: [0.938 0.934 0.932], greedy: [1. 1. 1.]\n",
      " 4/70 | (0, 0, 1) (*): actual: [0.66  0.688 0.806], greedy: [0.69  0.715 0.859]\n",
      " 4/70 | (0, 1, 0) (*): actual: [0.66  0.788 0.86 ], greedy: [0.698 0.824 0.924]\n",
      " 4/70 | (0, 1, 1) ( ): actual: [0.646 0.654 0.854], greedy: [0.657 0.672 0.904]\n",
      " 4/70 | (1, 0, 0) ( ): actual: [0.718 0.714 0.728], greedy: [0.746 0.737 0.78 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_train_mc):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m eval_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_eval_mc):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=10'>11</a>\u001b[0m         score, greedy_score \u001b[39m=\u001b[39m eval_online_q_learner(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=11'>12</a>\u001b[0m             train_patterns, test_pattern, train_id) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=12'>13</a>\u001b[0m         scores[train_patterns_id, test_pattern_id, train_id, eval_id] \u001b[39m=\u001b[39m score\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=13'>14</a>\u001b[0m         greedy_scores[train_patterns_id, test_pattern_id, train_id, eval_id] \u001b[39m=\u001b[39m greedy_score\n",
      "\u001b[1;32m/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb Cell 7\u001b[0m in \u001b[0;36meval_online_q_learner\u001b[0;34m(train_patterns, test_pattern, train_id)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=85'>86</a>\u001b[0m \u001b[39m# Give experience to learner and train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=86'>87</a>\u001b[0m learner\u001b[39m.\u001b[39mupdate_memory(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=87'>88</a>\u001b[0m     Transition(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=88'>89</a>\u001b[0m         obs_rep\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdetach(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=92'>93</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=93'>94</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=94'>95</a>\u001b[0m learner\u001b[39m.\u001b[39;49mtrain(done)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=96'>97</a>\u001b[0m \u001b[39m# Update next observation -> observation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhtc-g040/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/eval_experiments.ipynb#ch0000006vscode-remote?line=97'>98</a>\u001b[0m obs \u001b[39m=\u001b[39m next_obs\n",
      "File \u001b[0;32m/data/engs-oxfair3/orie4536/online-lever-adaptation/experiments/../levers/learner/dqn_agent.py:127\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, done)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m# Compute expected state action values\u001b[39;00m\n\u001b[1;32m    126\u001b[0m next_state_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(state_action_vals)\n\u001b[0;32m--> 127\u001b[0m next_state_vals[\u001b[39m~\u001b[39mdones,:] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_net(next_states[\u001b[39m~\u001b[39mdones,:])\u001b[39m.\u001b[39mmax(\n\u001b[1;32m    128\u001b[0m     \u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m    129\u001b[0m expected_state_action_vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_state_vals \u001b[39m+\u001b[39m rewards\n\u001b[1;32m    131\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "    (0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1),\n",
    "    (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1),\n",
    "]\n",
    "\n",
    "for train_patterns_id, train_patterns in enumerate(combinations(patterns, 4)):\n",
    "    print('-' * 50)\n",
    "    for test_pattern_id, test_pattern in enumerate(patterns):\n",
    "        for train_id in range(n_train_mc):\n",
    "            for eval_id in range(n_eval_mc):\n",
    "                score, greedy_score = eval_online_q_learner(\n",
    "                    train_patterns, test_pattern, train_id) \n",
    "                scores[train_patterns_id, test_pattern_id, train_id, eval_id] = score\n",
    "                greedy_scores[train_patterns_id, test_pattern_id, train_id, eval_id] = greedy_score\n",
    "        in_train_patterns = '*' if test_pattern in train_patterns else ' '\n",
    "        np.set_printoptions(precision=3)\n",
    "        print('{tps_id: 2d}/70 | {testp} ({flag}): actual: {actual}, greedy: {greedy}'.format(\n",
    "            tps_id=train_patterns_id+1,\n",
    "            testp=test_pattern,\n",
    "            flag=in_train_patterns,\n",
    "            actual=scores[train_patterns_id, test_pattern_id,:,:].mean(dim=1).numpy(),\n",
    "            greedy=greedy_scores[train_patterns_id, test_pattern_id,:,:].mean(dim=1).numpy(),\n",
    "        ))\n",
    "    avg_train_score = torch.stack([scores[train_patterns_id, pattern_id,:,:].mean() for pattern_id, pattern in enumerate(patterns) if pattern in train_patterns]).mean().item()\n",
    "    avg_test_scores = torch.stack([scores[train_patterns_id, pattern_id,:,:].mean() for pattern_id, pattern in enumerate(patterns) if pattern not in train_patterns]).mean().item()\n",
    "    avg_greedy_train_score = torch.stack([greedy_scores[train_patterns_id, pattern_id,:,:].mean() for pattern_id, pattern in enumerate(patterns) if pattern in train_patterns]).mean().item()\n",
    "    avg_greedy_test_scores = torch.stack([greedy_scores[train_patterns_id, pattern_id,:,:].mean() for pattern_id, pattern in enumerate(patterns) if pattern not in train_patterns]).mean().item()\n",
    "    print(f'Actual-train: {avg_train_score:5.2f}, Actual-test: {avg_test_scores:5.2f}')\n",
    "    print(f'Greedy-train: {avg_greedy_train_score:5.2f}, Greedy-test: {avg_greedy_test_scores:5.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
