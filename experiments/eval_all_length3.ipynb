{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import deque\n",
    "\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from levers import IteratedLeverEnvironment\n",
    "from levers.partners import FixedPatternPartner\n",
    "from levers.learner import DQNAgent, DRQNetwork, DRQNAgent, Transition\n",
    "from levers.helpers import generate_binary_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f171e2c7ad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class with dot notation access similar to wandb.config\n",
    "class WandbDict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODQL_agent_from_config(\n",
    "    env: IteratedLeverEnvironment, config: WandbDict\n",
    "):\n",
    "    # Create base class\n",
    "    hist_rep = nn.LSTM(\n",
    "        input_size=len(env.dummy_obs()),\n",
    "        hidden_size=config.hist_rep_dim\n",
    "    )\n",
    "    learner = DQNAgent(\n",
    "        q_net=nn.Sequential(\n",
    "            nn.Linear(config.hist_rep_dim, config.dqn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.dqn_hidden_dim, env.n_actions()),\n",
    "        ),\n",
    "        capacity=0,\n",
    "        batch_size=0,\n",
    "        lr=config.dqn_lr,\n",
    "        tau=config.dqn_tau,\n",
    "        len_update_cycle=config.dqn_len_update_cycle,\n",
    "        gamma=config.dqn_gamma,\n",
    "        use_running_memory=config.dqn_use_running_memory,\n",
    "    )\n",
    "\n",
    "    # Load parameters from disk\n",
    "    patterns = generate_binary_patterns(3)\n",
    "    train_configurations = list(combinations(patterns, 4))\n",
    "    train_patterns=train_configurations[config.exp_train_patterns_index]\n",
    "    train_seed=config.exp_train_seed\n",
    "\n",
    "    # Load history representation network's parameters\n",
    "    hist_rep_name = f\"odql_all_length3/models/hist-{train_patterns}-{train_seed}.pt\"\n",
    "    hist_rep.load_state_dict(torch.load(hist_rep_name))\n",
    "\n",
    "    # Load Q-network's parameters\n",
    "    q_net_name = f\"odql_all_length3/models/q-{train_patterns}-{train_seed}.pt\"\n",
    "    learner.q_net.load_state_dict(torch.load(q_net_name))\n",
    "\n",
    "    return hist_rep, learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DRQN_agent_from_config(\n",
    "    env: IteratedLeverEnvironment, config: WandbDict\n",
    "):\n",
    "    # Load base class\n",
    "    learner = DRQNAgent(\n",
    "        q_net=DRQNetwork(\n",
    "            rnn=nn.LSTM(\n",
    "                input_size=len(env.dummy_obs()),\n",
    "                hidden_size=config.drqn_rnn_hidden_dim,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            fnn=nn.Sequential(\n",
    "                nn.Linear(\n",
    "                    in_features=config.drqn_rnn_hidden_dim,\n",
    "                    out_features=config.drqn_fnn_hidden_dim,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(\n",
    "                    in_features=config.drqn_fnn_hidden_dim,\n",
    "                    out_features=env.n_actions(),\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        capacity=config.drqn_capacity,\n",
    "        batch_size=config.drqn_batch_size,\n",
    "        lr=config.drqn_lr,\n",
    "        tau=config.drqn_tau,\n",
    "        gamma=config.drqn_gamma,\n",
    "        len_update_cycle=config.drqn_len_update_cycle,\n",
    "    )\n",
    "\n",
    "    # Load parameters from disk\n",
    "    patterns = generate_binary_patterns(3)\n",
    "    train_configurations = list(combinations(patterns, 4))\n",
    "    train_patterns=train_configurations[config.exp_train_patterns_index]\n",
    "    train_seed=config.exp_train_seed\n",
    "\n",
    "    # Load Q-network's parameters\n",
    "    drqn_name = f\"drqn_all_length3/models/q-{train_patterns}-{train_seed}.pt\"\n",
    "    learner.q_net.load_state_dict(torch.load(drqn_name))\n",
    "\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ODQL_learner(config, env, eps_start, eps_n_decay_steps):\n",
    "    hist_rep, learner = ODQL_agent_from_config(env, config)\n",
    "\n",
    "    # Prepare stats\n",
    "    epi_return, epi_greedy_return, epi_n_greedy_steps = 0, 0, 0\n",
    "    last_actions = deque(maxlen=4)\n",
    "\n",
    "    # Reset environment and learner\n",
    "    learner.reset()\n",
    "    obs = env.reset()\n",
    "    obs_rep, hidden = hist_rep(obs.unsqueeze(0))\n",
    "    for step in range(env.episode_length - 1):\n",
    "        eps = eps_start * max(1 - step / eps_n_decay_steps, 0)\n",
    "        action, greedy = learner.act(obs_rep.squeeze(0), eps)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        next_obs_rep, next_hidden = hist_rep(next_obs.unsqueeze(0), hidden)\n",
    "        learner.update_memory(\n",
    "            Transition(\n",
    "                obs_rep.squeeze(0).detach(),\n",
    "                action,\n",
    "                next_obs_rep.squeeze(0).detach(),\n",
    "                reward,\n",
    "                done,\n",
    "            )\n",
    "        )\n",
    "        learner.train()\n",
    "        epi_return += reward\n",
    "        epi_greedy_return += reward if greedy else 0\n",
    "        epi_n_greedy_steps += 1 if greedy else 0\n",
    "        last_actions.append(action if greedy else -1)\n",
    "        obs_rep = next_obs_rep\n",
    "        hidden = next_hidden\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'return': epi_return,\n",
    "        'normalized_greedy_return': epi_greedy_return / epi_n_greedy_steps * 100,\n",
    "        'greedy_return': epi_greedy_return,\n",
    "        'n_greedy_steps': epi_n_greedy_steps,\n",
    "        'last_actions': last_actions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_DRQN_learner(config, env):\n",
    "    learner = DRQN_agent_from_config(env, config)\n",
    "\n",
    "    # Prepare stats\n",
    "    epi_return = 0\n",
    "    last_actions = deque(maxlen=4)\n",
    "\n",
    "    # Reset environment and learner\n",
    "    obs = env.reset()\n",
    "    learner.reset_new_episode(init_obs=obs)\n",
    "    for _ in range(env.episode_length - 1):\n",
    "        action = learner.act(obs)\n",
    "        last_actions.append(action)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        obs = next_obs\n",
    "        epi_return += reward\n",
    "\n",
    "    return {\n",
    "        'return': epi_return,\n",
    "        'last_actions': last_actions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ODQL_crossplay(config1, config2, eps_start, eps_n_decay_steps):\n",
    "    env = IteratedLeverEnvironment(\n",
    "        payoffs=[1., 1.0],\n",
    "        n_iterations=100+1,\n",
    "        include_step=False,\n",
    "        include_payoffs=False,\n",
    "    )\n",
    "\n",
    "    hist_rep1, agent1 = ODQL_agent_from_config(env, config1)\n",
    "    hist_rep2, agent2 = ODQL_agent_from_config(env, config2)\n",
    "\n",
    "    # Prepare stats\n",
    "    epi_return, epi_greedy_return, epi_n_greedy_steps = 0, 0, 0\n",
    "    last_actions = deque(maxlen=4)\n",
    "\n",
    "    joint_obs = env.reset()\n",
    "\n",
    "    agent1.reset()\n",
    "    agent2.reset()\n",
    "\n",
    "    obs_rep1, hidden1 = hist_rep1(joint_obs[0].unsqueeze(0))\n",
    "    obs_rep2, hidden2 = hist_rep2(joint_obs[1].unsqueeze(0))\n",
    "\n",
    "    for step in range(env.episode_length - 1):\n",
    "        # Get agents' actions\n",
    "        eps = eps_start * max(1 - step / eps_n_decay_steps, 0)\n",
    "        action1, greedy1 = agent1.act(obs_rep1.squeeze(0), eps)\n",
    "        action2, greedy2 = agent2.act(obs_rep2.squeeze(0), eps)\n",
    "\n",
    "        next_joint_obs, reward, done = env.step([action1, action2])\n",
    "\n",
    "        next_obs_rep1, hidden1 = hist_rep1(next_joint_obs[0].unsqueeze(0), hidden1)\n",
    "        next_obs_rep2, hidden2 = hist_rep2(next_joint_obs[1].unsqueeze(0), hidden2)\n",
    "\n",
    "        # Train agent 1\n",
    "        agent1.update_memory(\n",
    "            Transition(\n",
    "                obs_rep1.squeeze(0).detach(),\n",
    "                action1,\n",
    "                next_obs_rep1.squeeze(0).detach(),\n",
    "                reward,\n",
    "                done,\n",
    "            )\n",
    "        )\n",
    "        agent1.train()\n",
    "\n",
    "        # Train agent 2\n",
    "        agent2.update_memory(\n",
    "            Transition(\n",
    "                obs_rep2.squeeze(0).detach(),\n",
    "                action2,\n",
    "                next_obs_rep2.squeeze(0).detach(),\n",
    "                reward,\n",
    "                done,\n",
    "            )\n",
    "        )\n",
    "        agent2.train()\n",
    "\n",
    "        # Update stats\n",
    "        epi_return += reward\n",
    "        epi_greedy_return += reward if greedy1 and greedy2 else 0\n",
    "        epi_n_greedy_steps += 1 if greedy1 and greedy2 else 0\n",
    "\n",
    "        # Prepare next step\n",
    "        obs_rep1 = next_obs_rep1\n",
    "        obs_rep2 = next_obs_rep2\n",
    "\n",
    "    return {\n",
    "        'return': epi_return,\n",
    "        'normalized_greedy_return': epi_greedy_return / epi_n_greedy_steps * 100,\n",
    "        'greedy_return': epi_greedy_return,\n",
    "        'n_greedy_steps': epi_n_greedy_steps,\n",
    "        'last_actions': last_actions\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = generate_binary_patterns(3)\n",
    "train_configurations = list(combinations(patterns, 4))\n",
    "\n",
    "envs = [\n",
    "    IteratedLeverEnvironment(\n",
    "        payoffs=[1., 1.],\n",
    "        n_iterations=100 + 1,\n",
    "        partner=FixedPatternPartner(pattern),\n",
    "        include_step=False,\n",
    "        include_payoffs=False,\n",
    "    )\n",
    "    for pattern in patterns\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODQL Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=60)\n",
    "\n",
    "# Obtain runs from ODQL parameter sweep\n",
    "runs = api.runs(\"hericks/odql-all-length3\")\n",
    "\n",
    "# Obtain parameter configs for each run\n",
    "configs = []\n",
    "for run in runs: \n",
    "    config = {k:v for k,v in run.config.items() if not k.startswith('_')}\n",
    "    configs.append(WandbDict(config))\n",
    "\n",
    "# Obtain number of training seeds per configuration\n",
    "n_train_seeds = max([config.exp_train_seed for config in configs]) + 1\n",
    "print(f\"The ODQL agents were trained for {n_train_seeds} different seeds each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for config_index, config in enumerate(configs):\n",
    "    if (config_index + 1) % 200 == 0:\n",
    "        print(config_index + 1)\n",
    "    for eval_index, eval_pattern in enumerate(patterns):\n",
    "        results = eval_ODQL_learner(\n",
    "            config=config,\n",
    "            env=envs[eval_index],\n",
    "            eps_start=1.0,\n",
    "            eps_n_decay_steps=33,\n",
    "        )\n",
    "        results.update({\n",
    "            'method': 'ODQL',\n",
    "            'train_patterns': train_configurations[config.exp_train_patterns_index],\n",
    "            'train_seed': config.exp_train_seed,\n",
    "            'eval_pattern': eval_pattern,\n",
    "            'seen_at_train_time': eval_pattern in train_configurations[config.exp_train_patterns_index],\n",
    "        })\n",
    "        results_list.append(results)\n",
    "\n",
    "odql_results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odql_results_df = odql_results_df[['method', 'train_patterns', 'train_seed', 'eval_pattern', 'seen_at_train_time', 'return', 'normalized_greedy_return', 'greedy_return', 'n_greedy_steps', 'last_actions']]\n",
    "odql_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = odql_results_df['seen_at_train_time']\n",
    "print(f\"Overall: {odql_results_df['return'].mean():.2f} | {odql_results_df['normalized_greedy_return'].mean():.2f}\")\n",
    "print(f\"Train: {odql_results_df[train]['return'].mean():.2f} | {odql_results_df[train]['normalized_greedy_return'].mean():.2f}\")\n",
    "print(f\"Test: {odql_results_df[~train]['return'].mean():.2f} | {odql_results_df[~train]['normalized_greedy_return'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DRQN Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=60)\n",
    "\n",
    "# Obtain runs from ODQL parameter sweep\n",
    "runs = api.runs(\"hericks/drqn-all-length3\")\n",
    "\n",
    "# Obtain parameter configs for each run\n",
    "configs = []\n",
    "for run in runs: \n",
    "    config = {k:v for k,v in run.config.items() if not k.startswith('_')}\n",
    "    configs.append(WandbDict(config))\n",
    "\n",
    "# Obtain number of training seeds per configuration\n",
    "n_train_seeds = max([config.exp_train_seed for config in configs]) + 1\n",
    "print(f\"The DRQN agents were trained for {n_train_seeds} different seeds each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for config_index, config in enumerate(configs):\n",
    "    if (config_index + 1) % 200 == 0:\n",
    "        print(config_index + 1)\n",
    "    for eval_index, eval_pattern in enumerate(patterns):\n",
    "        try:\n",
    "            results = eval_DRQN_learner(\n",
    "                config=config,\n",
    "                env=envs[eval_index],\n",
    "            )\n",
    "            results.update({\n",
    "                'method': 'DRQN',\n",
    "                'train_patterns': train_configurations[config.exp_train_patterns_index],\n",
    "                'train_seed': config.exp_train_seed,\n",
    "                'eval_pattern': eval_pattern,\n",
    "                'seen_at_train_time': eval_pattern in train_configurations[config.exp_train_patterns_index],\n",
    "            })\n",
    "            results_list.append(results)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "drqn_results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drqn_results_df = drqn_results_df[['method', 'train_patterns', 'train_seed', 'eval_pattern', 'seen_at_train_time', 'return', 'last_actions']]\n",
    "drqn_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = drqn_results_df['seen_at_train_time']\n",
    "print(f\"Overall: {drqn_results_df['return'].mean():.2f}\")\n",
    "print(f\"Train: {drqn_results_df[train]['return'].mean():.2f}\")\n",
    "print(f\"Test: {drqn_results_df[~train]['return'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Crossplay Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = generate_binary_patterns(3)\n",
    "all_train_configurations = list(combinations(patterns, 4))\n",
    "n_train_configurations = len(all_train_configurations)\n",
    "\n",
    "n_xplay = 15\n",
    "xplay_train_configurations_indices = random.sample(range(n_train_configurations), n_xplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODQL Crossplay Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=60)\n",
    "\n",
    "needed_dict = {\n",
    "    i: False if i not in xplay_train_configurations_indices else True\n",
    "    for i in range(n_train_configurations)\n",
    "}\n",
    "\n",
    "# Obtain runs from ODQL parameter sweep\n",
    "runs = api.runs(\"hericks/odql-all-length3\")\n",
    "\n",
    "# Obtain parameter configs for each run\n",
    "crossplay_configs = []\n",
    "for run in runs: \n",
    "    config = WandbDict({k:v for k,v in run.config.items() if not k.startswith('_')})\n",
    "    if needed_dict[config.exp_train_patterns_index]:\n",
    "        crossplay_configs.append(WandbDict(config))\n",
    "        needed_dict[config.exp_train_patterns_index] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "rows_list = []\n",
    "for i in range(n_xplay):\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(i + 1)\n",
    "    for j in range(n_xplay):\n",
    "        res = eval_ODQL_crossplay(crossplay_configs[0], crossplay_configs[1], 0.5, 30)\n",
    "        res.update({\n",
    "            \"i1\": i,\n",
    "            \"i2\": j,\n",
    "            \"i1_train_patterns\": all_train_configurations[crossplay_configs[i].exp_train_patterns_index],\n",
    "            \"i2_train_patterns\": all_train_configurations[crossplay_configs[j].exp_train_patterns_index],\n",
    "        })\n",
    "        rows_list.append(res)\n",
    "\n",
    "odql_xplay_df = pd.DataFrame(rows_list)\n",
    "odql_xplay_df = odql_xplay_df[['i1', 'i2', 'i1_train_patterns', 'i2_train_patterns', 'return', 'normalized_greedy_return', 'greedy_return', 'n_greedy_steps', 'last_actions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i1</th>\n",
       "      <th>i2</th>\n",
       "      <th>i1_train_patterns</th>\n",
       "      <th>i2_train_patterns</th>\n",
       "      <th>return</th>\n",
       "      <th>normalized_greedy_return</th>\n",
       "      <th>greedy_return</th>\n",
       "      <th>n_greedy_steps</th>\n",
       "      <th>last_actions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.910112</td>\n",
       "      <td>64.0</td>\n",
       "      <td>89</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>((0, 1, 1), (1, 0, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.325581</td>\n",
       "      <td>45.0</td>\n",
       "      <td>86</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>((0, 1, 0), (1, 0, 0), (1, 0, 1), (1, 1, 0))</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.888889</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>((0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>90.0</td>\n",
       "      <td>98.837209</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>((0, 0, 1), (0, 1, 1), (1, 1, 0), (1, 1, 1))</td>\n",
       "      <td>87.0</td>\n",
       "      <td>94.047619</td>\n",
       "      <td>79.0</td>\n",
       "      <td>84</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (1, 0, 1), (1, 1, 1))</td>\n",
       "      <td>83.0</td>\n",
       "      <td>85.542169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>83</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 0, 1))</td>\n",
       "      <td>89.0</td>\n",
       "      <td>92.222222</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 1, 1))</td>\n",
       "      <td>94.0</td>\n",
       "      <td>98.863636</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0))</td>\n",
       "      <td>63.0</td>\n",
       "      <td>64.772727</td>\n",
       "      <td>57.0</td>\n",
       "      <td>88</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))</td>\n",
       "      <td>((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))</td>\n",
       "      <td>89.0</td>\n",
       "      <td>93.258427</td>\n",
       "      <td>83.0</td>\n",
       "      <td>89</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     i1  i2                             i1_train_patterns  \\\n",
       "0     0   0  ((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))   \n",
       "1     0   1  ((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))   \n",
       "2     0   2  ((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))   \n",
       "3     0   3  ((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))   \n",
       "4     0   4  ((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))   \n",
       "..   ..  ..                                           ...   \n",
       "220  14  10  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))   \n",
       "221  14  11  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))   \n",
       "222  14  12  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))   \n",
       "223  14  13  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))   \n",
       "224  14  14  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))   \n",
       "\n",
       "                                i2_train_patterns  return  \\\n",
       "0    ((1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1))    70.0   \n",
       "1    ((0, 1, 1), (1, 0, 1), (1, 1, 0), (1, 1, 1))    52.0   \n",
       "2    ((0, 1, 0), (1, 0, 0), (1, 0, 1), (1, 1, 0))    96.0   \n",
       "3    ((0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1))    90.0   \n",
       "4    ((0, 0, 1), (0, 1, 1), (1, 1, 0), (1, 1, 1))    87.0   \n",
       "..                                            ...     ...   \n",
       "220  ((0, 0, 0), (0, 0, 1), (1, 0, 1), (1, 1, 1))    83.0   \n",
       "221  ((0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 0, 1))    89.0   \n",
       "222  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 1, 1))    94.0   \n",
       "223  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0))    63.0   \n",
       "224  ((0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1))    89.0   \n",
       "\n",
       "     normalized_greedy_return  greedy_return  n_greedy_steps last_actions  \n",
       "0                   71.910112           64.0              89           []  \n",
       "1                   52.325581           45.0              86           []  \n",
       "2                   98.888889           89.0              90           []  \n",
       "3                   98.837209           85.0              86           []  \n",
       "4                   94.047619           79.0              84           []  \n",
       "..                        ...            ...             ...          ...  \n",
       "220                 85.542169           71.0              83           []  \n",
       "221                 92.222222           83.0              90           []  \n",
       "222                 98.863636           87.0              88           []  \n",
       "223                 64.772727           57.0              88           []  \n",
       "224                 93.258427           83.0              89           []  \n",
       "\n",
       "[225 rows x 9 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odql_xplay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODQL Crossplay: 83.51 | 88.54 (greedy)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ODQL Crossplay: {odql_xplay_df['return'].mean():.2f} | {odql_xplay_df['normalized_greedy_return'].mean():.2f} (greedy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "odql_xplay_returns_matrix = np.zeros((n_xplay, n_xplay))\n",
    "odql_xplay_normalized_greedy_returns_matrix = np.zeros((n_xplay, n_xplay))\n",
    "for i in range(n_xplay):\n",
    "    for j in range(n_xplay):\n",
    "        row_mask = (odql_xplay_df['i1'] == i) & (odql_xplay_df['i2'] == j)\n",
    "        odql_xplay_returns_matrix[i, j] = odql_xplay_df[row_mask]['return'].item()\n",
    "        odql_xplay_normalized_greedy_returns_matrix[i, j] = odql_xplay_df[row_mask]['normalized_greedy_return'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1713a62820>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(odql_xplay_returns_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f17139dc700>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(odql_xplay_normalized_greedy_returns_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def test(eps_start, eps_n_decay_steps):\n",
    "    results_list = []\n",
    "    for config_index, config in enumerate(configs):\n",
    "        if (config_index + 1) % 50 == 0:\n",
    "            print(config_index + 1)\n",
    "        for eval_index, eval_pattern in enumerate(patterns):\n",
    "            results = eval_ODQL_learner(\n",
    "                train_patterns=train_configurations[config.exp_train_patterns_index],\n",
    "                train_seed=config.exp_train_seed,\n",
    "                config=config,\n",
    "                env=envs[eval_index],\n",
    "                eps_start=eps_start,\n",
    "                eps_n_decay_steps=eps_n_decay_steps,\n",
    "            )\n",
    "            results.update({\n",
    "                'method': 'ODQL',\n",
    "                'train_patterns': train_configurations[config.exp_train_patterns_index],\n",
    "                'train_seed': config.exp_train_seed,\n",
    "                'eval_pattern': eval_pattern,\n",
    "                'seen_at_train_time': eval_pattern in train_configurations[config.exp_train_patterns_index],\n",
    "            })\n",
    "            results_list.append(results)\n",
    "\n",
    "        if config_index > 50:\n",
    "            break\n",
    "\n",
    "    odql_results_df = pd.DataFrame(results_list)\n",
    "    odql_results_df = odql_results_df[['method', 'train_patterns', 'train_seed', 'eval_pattern', 'seen_at_train_time', 'return', 'normalized_greedy_return', 'greedy_return', 'n_greedy_steps', 'last_actions']]\n",
    "    train = odql_results_df['seen_at_train_time']\n",
    "    print(f\"Overall: {odql_results_df['return'].mean():.2f} | {odql_results_df['normalized_greedy_return'].mean():.2f}\")\n",
    "    print(f\"Train: {odql_results_df[train]['return'].mean():.2f} | {odql_results_df[train]['normalized_greedy_return'].mean():.2f}\")\n",
    "    print(f\"Test: {odql_results_df[~train]['return'].mean():.2f} | {odql_results_df[~train]['normalized_greedy_return'].mean():.2f}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "params = [\n",
    "    (1.0, 10), (1.0, 20), (1.0, 30), (1.0, 40), (1.0, 50),\n",
    "    (0.5, 10), (0.5, 20), (0.5, 30), (0.5, 40), (0.5, 50),\n",
    "]\n",
    "for param in params:\n",
    "    print(param[0], param[1])\n",
    "    test(param[0], param[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('evotorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93fe7ab9e9c894208fb85df19994ba2ccb8ea74b824756365ece5b2ab999c742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
