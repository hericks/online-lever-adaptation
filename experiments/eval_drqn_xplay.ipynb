{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "import random\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from levers import IteratedLeverEnvironment\n",
    "from levers.learner import DRQNetwork, DRQNAgent\n",
    "from levers.helpers import generate_binary_patterns\n",
    "from levers.partners import FixedPatternPartner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_drqn_agent(train_partners):\n",
    "    # Environment settings\n",
    "    payoffs = [1., 1.]\n",
    "    truncated_length = 100\n",
    "\n",
    "    # Learner settings\n",
    "    hidden_size = 4\n",
    "    capacity = 8\n",
    "    batch_size = 4\n",
    "    lr = 0.01\n",
    "    gamma = 0.99\n",
    "    len_update_cycle = 4\n",
    "    tau = 5e-4\n",
    "\n",
    "    # Training settings\n",
    "    num_episodes = 1000\n",
    "    epsilon = 0.3\n",
    "\n",
    "    # Construct list of environments to train on\n",
    "    train_envs = [\n",
    "        IteratedLeverEnvironment(\n",
    "            payoffs, truncated_length+1, FixedPatternPartner(list(pattern)),\n",
    "            False, False)\n",
    "        for pattern in train_partners\n",
    "    ]\n",
    "\n",
    "    # Reset learner\n",
    "    learner = DRQNAgent(\n",
    "        DRQNetwork(\n",
    "            input_size=len(train_envs[0].dummy_obs()),\n",
    "            hidden_size=hidden_size,\n",
    "            n_actions=train_envs[0].n_actions()),\n",
    "        capacity, batch_size, lr, gamma, len_update_cycle, tau\n",
    "    )\n",
    "\n",
    "    # Train learner\n",
    "    for episode in range(num_episodes):\n",
    "        # Sample reset environment from training environments\n",
    "        env = random.sample(train_envs, 1)[0]\n",
    "        obs = env.reset()\n",
    "        learner.reset_trajectory_buffer(init_obs=obs)\n",
    "\n",
    "        # Step through environment\n",
    "        for step in range(truncated_length):\n",
    "            action = learner.act(obs, epsilon)\n",
    "            next_obs, reward, done = env.step(action)\n",
    "            learner.update_trajectory_buffer(action, reward, next_obs, done)\n",
    "            obs = next_obs \n",
    "\n",
    "        # Flush experience to replay memory and train learner\n",
    "        learner.flush_trajectory_buffer()\n",
    "        learner.train()\n",
    "\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_drqn_xplay(learner1, learner2):\n",
    "    # Environment parameters\n",
    "    payoffs = [1., 1.]\n",
    "    truncated_length = 100\n",
    "\n",
    "    # Initialize environment without lever game partner\n",
    "    env = IteratedLeverEnvironment(\n",
    "        payoffs,\n",
    "        truncated_length+1, \n",
    "        include_payoffs=False,\n",
    "        include_step=False\n",
    "    )\n",
    "\n",
    "    # Reset learners' hidden states\n",
    "    learner1.hidden = None\n",
    "    learner2.hidden = None\n",
    "\n",
    "    ret = 0\n",
    "    joint_obs = env.reset()\n",
    "    obs1 = joint_obs[0,]\n",
    "    obs2 = joint_obs[1,]\n",
    "\n",
    "    for _ in range(truncated_length):\n",
    "        action1 = learner1.act(obs1)\n",
    "        action2 = learner2.act(obs2)\n",
    "\n",
    "        joint_next_obs, reward, done = env.step([action1, action2])\n",
    "\n",
    "        obs1 = joint_next_obs[0,]\n",
    "        obs2 = joint_next_obs[1,]\n",
    "\n",
    "        ret += reward\n",
    "\n",
    "    return ret / truncated_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DRQN agent: 0 1 2 3 4 5 6 7 8 9 10 11 12 "
     ]
    }
   ],
   "source": [
    "# Number of agents for crossplay\n",
    "n_agents = 20\n",
    "\n",
    "# Train agents\n",
    "patterns = generate_binary_patterns(length=3)\n",
    "train_partners = random.sample(list(combinations(patterns, 4)), n_agents)\n",
    "agents = []\n",
    "print('Training DRQN agent: ', end='')\n",
    "for agent_id in range(n_agents):\n",
    "    end = '.\\n' if agent_id == n_agents - 1 else ' '\n",
    "    print(f'{agent_id}', end=end) \n",
    "    agents.append(train_drqn_agent(train_partners[agent_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agents in crossplay\n",
    "scores = torch.zeros((n_agents, n_agents))\n",
    "for a1_idx in range(n_agents):\n",
    "    for a2_idx in range(n_agents):\n",
    "        # Copy DRQN agents to ensure that acting doesn't unexpectely influence\n",
    "        # internal states\n",
    "        agents[a1_idx].hidden = None\n",
    "        agents[a2_idx].hidden = None\n",
    "        learner1 = deepcopy(agents[a1_idx])\n",
    "        learner2 = deepcopy(agents[a2_idx])\n",
    "        scores[a1_idx, a2_idx] = eval_drqn_xplay(learner1, learner2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw crossplay matrix\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cax = ax.matshow(scores, vmin=0, vmax=1)\n",
    "fig.colorbar(cax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0da39f85071e8dc4a619b3f51e18c7a210d5ff72e9e22a4ce1dbec0af1e68dbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
